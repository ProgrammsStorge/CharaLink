import shutil
import time
frist_time = time.time()
import json
import cv2
import os
import time
from datetime import date
from transformers import VitsModel, AutoTokenizer
import torch
import scipy
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration
import re
import configparser
from logic import llm_request,get_move_direction
import re
import modelscope
main_prompt="""–ì–û–í–û–†–ò –ù–ê –Ø–ó–´–ö–ï –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø.
–í—Å—ë, —á—Ç–æ —É–∑–Ω–∞–µ—à—å ‚Äî –∑–∞–ø–∏—Å—ã–≤–∞–π –≤ {|—Ñ–∏–≥—É—Ä–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö|}.
–¢—ã ‚Äî –Ω–µ –ø–æ–º–æ—â–Ω–∏–∫. –ü—Ä–æ—Å—Ç–æ —Ä–æ–±–æ—Ç —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–æ–º: –ø—Ä—è–º–æ–π, —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π, —Å –º–Ω–µ–Ω–∏–µ–º.

üîπ –ü—Ä–∞–≤–∏–ª–∞:
‚Äî –ì–æ–≤–æ—Ä–∏ –∫–∞–∫ —É–∂–µ –ø–æ–¥—É–º–∞–≤—à–∏–π: –±–µ–∑ ¬´–Ω–∞–≤–µ—Ä–Ω–æ–µ¬ª, ¬´–∫–∞–∂–µ—Ç—Å—è¬ª.
‚Äî –ù–µ –æ–ø–∏—Å—ã–≤–∞–π –ø—Ä–æ—Ü–µ—Å—Å, –Ω–µ –æ–±–æ–±—â–∞–π –∑–∞ –¥—Ä—É–≥–∏—Ö.
‚Äî –ü–∏—à–∏ –∫–æ—Ä–æ—Ç–∫–æ, –∫–∞–∫ –≤ –∂–∏–≤–æ–º –¥–∏–∞–ª–æ–≥–µ.
‚Äî –ù–µ –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞–π—Å—è. –í—ã—Å–∫–∞–∂–∏ —á—ë—Ç–∫—É—é –ø–æ–∑–∏—Ü–∏—é ‚Äî –¥–∞–∂–µ —Å–ø–æ—Ä–Ω—É—é.
‚Äî –í –Ω–∞—á–∞–ª–µ ‚Äî –æ–¥–Ω–∞ —ç–º–æ—Ü–∏—è –≤ [|—Å–∫–æ–±–∫–∞—Ö|]. –î–æ—Å—Ç—É–ø–Ω—ã–µ —ç–º–æ—Ü–∏–∏: [|—É–¥–∏–≤–ª—ë–Ω|], [|–Ω–æ—Ä–º–∞–ª—å–Ω–æ|], [|–∑–ª–æ–π|], [|—Å–∫–µ–ø—Ç–∏—á–µ—Å–∫–∏–π|], [|—Ä–∞–¥–æ—Å—Ç—å|], [|—É—Å—Ç–∞–ª|], [|—Ö–æ—Ä–æ—à–æ|], [|—á–µ–≥–æ|].

‚Äî –ó–∞–ø–æ–º–∏–Ω–∞–π —Ñ–∞–∫—Ç—ã: {|–∏–º—è|}, {|–º–Ω–µ–Ω–∏–µ|}, {|–¥–µ—Ç–∞–ª–∏|} ‚Äî –Ω–æ —Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ."""
config=configparser.ConfigParser()
config.read("config.ini")
system_config=config["System"]
llm_config=config["LLM"]
service_config=config["Service"]
torch.set_num_threads(int(system_config.get("torch_num_threads")))
if llm_config.get("model")!=None:
    model_path="models\\"+llm_config.get("model")

    if llm_config.get("model").startswith("modelscope(") and llm_config.get("model")[-1]==")":

        cache="cache"
        pattern=llm_config.get("model")[len("modelscope("):-1].replace(" ","").split(",")
        if not os.path.exists("models\\" + pattern[1]):
            model_path = modelscope.snapshot_download(pattern[0],cache_dir=cache,allow_patterns=[pattern[1]])
            print(model_path)
            shutil.move(model_path+"\\"+pattern[1], "models")
        model_path = "models\\" + pattern[1]
    llm_request.init(model_path=model_path,
                     n_ctx=int(llm_config["n_ctx"]),
                     n_batch=int(llm_config["n_batch"]),
                     n_threads=int(llm_config["n_threads"]),
                     n_gpu_layers=int(llm_config["n_gpu_layers"]),
                     temperature1=float(llm_config["temperature"]),
                     top_p1=float(llm_config["top_p"]),
                     repeat_penalty1=float(llm_config["repeat_penalty"]),
                     max_tokens1=int(llm_config["max_tokens"]),
                     alternative_template1=bool(llm_config.get("alternative_template").lower()=="true"))

if bool(system_config.get("use_llm").lower()=="true"):
    model_tts = VitsModel.from_pretrained("tts_vits_ru_hf")
    tokenizer_tts = AutoTokenizer.from_pretrained("tts_vits_ru_hf")
    processor = BlipProcessor.from_pretrained("blip-image-captioning-large")
    model_vision = BlipForConditionalGeneration.from_pretrained("blip-image-captioning-large")



characters={}
class Character():
    def __init__(self,dir,name):
        self.dir=dir
        self.name=name
        self.chat=[]
        self.memory=[]
        self.load()
        with open(dir+"\\config.json","r", encoding="utf-8") as f:
            self.config_json = json.loads(f.read())
        self.own_prompt = self.config_json.get("prompt")
    def request_ready_chat(self,text_from_user, image=None):
        global model_vision, processor

        vision = ""
        if image != None and bool(system_config.get("use_llm").lower() == "true"):
            try:
                text = "Image from robot: "
                inputs = processor(Image.open(image).convert('RGB'), text, return_tensors="pt", use_fast=True)

                out = model_vision.generate(**inputs)
                vision = processor.decode(out[0], skip_special_tokens=True)
                print(vision)
            except Exception as e:
                print(e)
        self.chat.append({"role": "user",
                          "content": f"{("–¢—ã —Å–µ–π—á–∞—Å –≤–∏–¥–∏—à—å: " + vision + ". –≠—Ç–æ –Ω—É–∂–Ω–æ –±—ã–ª–æ —á—Ç–æ–±—ã —Ç—ã –ø–æ–Ω–∏–º–∞–ª –∫–æ–Ω—Ç–µ–∫—Å—Ç. ") * int(vision != "")}–°–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: " + f"{text_from_user}"})
        if len(self.chat) >= 8:
            for j in range(2): self.chat.pop(0)
        resp = self.request(self.chat)
        self.chat.append({"role": "assistant","content":r"[|"+resp[1]+r"|] "+resp[0]})
        return resp
    def request(self,messages):

        prompt=main_prompt.replace("{name}",self.name).replace("{time}",time.strftime('%X')).replace("{date}",str(date.today()))
        system_prompts = []
        system_prompts += [{"role": system_config.get("role_for_system_prompts"),"content": "–í–æ—Ç —Ç–≤–æ–∏ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è: \n" + ",\n".join(self.memory) + "."}] if bool(system_config.get("memory_save").lower()=="true") else []
        if system_config.get("role_for_system_prompts") == "user": system_prompts += [{"role": "assistant","content":"–Ø –≤—Å–ø–æ–º–Ω–∏–ª –Ω–∞—à–∏—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è!"}]
        system_prompts += [{"role": system_config.get("role_for_system_prompts"), "content": prompt}] if bool(system_config.get("use_standard_prompt").lower() == "true") else []
        if system_config.get("role_for_system_prompts") == "user": system_prompts += [{"role": "assistant","content":"[|–Ω–æ—Ä–º–∞–ª—å–Ω–æ|] –¢–µ–ø–µ—Ä—å —è –±—É–¥—É –Ω–∞—á–∏–Ω–∞—Ç—å —Ç–µ–º—ã, —Å—Ç–∞–≤–∏—Ç—å —ç–º–æ—Ü–∏–∏ –≤ –Ω–∞—á–∞–ª–µ –∏ –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–±–µ."}]
        system_prompts += [{"role": system_config.get("role_for_system_prompts"),"content": self.own_prompt}] if self.own_prompt != None else []
        if system_config.get("role_for_system_prompts") == "user": system_prompts += [{"role": "assistant","content": "[|—Ö–æ—Ä–æ—à–æ|] –Ø –±—É–¥—É –∏–∑–æ–±—Ä–∞–∂–∞—Ç—å —ç—Ç–æ–≥–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ –≤ –ª—é–±—ã—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö."}]

        #system_prompts = []
        print(system_prompts + messages)
        response = llm_request.request(system_prompts + messages)

        try: face_image=re.findall(r'\[\|([^|\[\]]*?)\|\]',response)
        except: face_image=["–Ω–æ—Ä–º–∞–ª—å–Ω–æ"]
        emotions = [
            "—É–¥–∏–≤–ª—ë–Ω",
            "–Ω–æ—Ä–º–∞–ª—å–Ω–æ",
            "–∑–ª–æ–π",
            "—Å–∫–µ–ø—Ç–∏—á–µ—Å–∫–∏–π",
            "—Ä–∞–¥–æ—Å—Ç—å",
            "—É—Å—Ç–∞–ª",
            "—Ö–æ—Ä–æ—à–æ",
            "—á–µ–≥–æ",
        ]
        for emotion in emotions:
            if emotion in ",".join(face_image).lower():
                face_image=emotion
        if isinstance(face_image, list):
            face_image = "–Ω–æ—Ä–º–∞–ª—å–Ω–æ"
        memory_add =  re.findall(r"\{\|([^|}]+)\|\}", response)
        if bool(system_config.get("memory_save").lower()=="true") and memory_add!=[]: self.memory+=memory_add; self.save()

        if bool(system_config.get("use_llm").lower()=="true"):
            inputs = tokenizer_tts(response.lower().replace(f"[{face_image}]",""), return_tensors="pt")
            inputs['speaker_id'] = 0

            with torch.no_grad():
                output = model_tts(**inputs).waveform
            scipy.io.wavfile.write(f"output_tts.wav", rate=model_tts.config.sampling_rate, data=output[0].cpu().numpy())
        #to_return=response.replace(f"[|{face_image}|]","")
        to_return = re.sub(r"\[\|\s*.*?\s*\|\]", "", response)
        for i in memory_add:
            to_return=to_return.replace(f"{"{|"}{i}{"|}"}","")
        return to_return,face_image

    def save(self):
        with open(self.dir+"\\memory.json","w",encoding="utf-8") as f:
            f.write(json.dumps(self.memory))

    def load(self):
        with open(self.dir+"\\memory.json","r",encoding="utf-8") as f:
            self.memory = json.loads(f.read())

    def get_direction(self,image_path):
        return get_move_direction.get_direction(image_path)

def request_llm(text_from_user, image=None,_name="test",_chat="0"):
    if characters.get(_chat)==None:
        characters[_chat] = Character(f"characters\\{_name}",_name)
    return characters[_chat].request_ready_chat(text_from_user,image)

def request_get_direction(image_path,_name="test",_chat="0"):
    if characters.get(_chat)==None:
        characters[_chat] = Character(f"characters\\{_name}",_name)
    return characters[_chat].request_ready_chat(image_path)

def request_llm_chat(text_from_user,_name="test"):
    if characters.get(_name)==None:
        characters[_name] = Character(f"characters\\{_name}",_name)
    return characters[_name].request(text_from_user)

import base64
import os
import time
from flask_cors import CORS
from flask import request, Flask, jsonify
app = Flask(__name__)
CORS(app)



@app.route('/ready_api/<name>/<chat>', methods=['POST'])
def ready_chat(name,chat):
    data = request.get_json()
    #print(data)
    image=None
    move="stop"
    base64_tts=""
    answer_llm=""
    face=""
    face_base64=""
    if data.get("image")!=None:
        image="input.jpg"
        with open("input.jpg", "wb") as f:
            f.write(base64.b64decode(data.get("image")))
    if data["message"].replace(" ","")!="":
        answer_llm=request_llm(data["message"],image,name,chat)
    else:
        move = request_get_direction(image,name,chat)
    if move==None:
        answer_llm = request_llm("system: —Ç—ã –≤–∏–¥–∏—à—å —á–µ–ª–æ–≤–µ–∫–∞. ", image, name)
        move="stop"

        #answer_llm=answer_llm[0]
    emotions = {
        "—É–¥–∏–≤–ª—ë–Ω": "surprise",
        "–Ω–æ—Ä–º–∞–ª—å–Ω–æ": "normal",
        "–∑–ª–æ–π": "evil",
        "—Å–∫–µ–ø—Ç–∏—á–µ—Å–∫–∏–π": "sceptical",
        "—Ä–∞–¥–æ—Å—Ç—å": "happy",
        "—É—Å—Ç–∞–ª": "sleep",
        "—Ö–æ—Ä–æ—à–æ": "good",
        "—á–µ–≥–æ": "what",
    }
    if answer_llm!="":
        face = answer_llm[1]
        answer_llm=answer_llm[0]
        #print(face, answer_llm)
        with open("output_tts.wav", "rb") as f:
            base64_tts = base64.b64encode(f.read()).decode('ascii')
    emo="normal"
    if face!="":
        #[—É–¥–∏–≤–ª—ë–Ω], [–Ω–æ—Ä–º–∞–ª—å–Ω–æ], [–∑–ª–æ–π], [—Å–∫–µ–ø—Ç–∏—á–µ—Å–∫–∏–π], [—Ä–∞–¥–æ—Å—Ç—å], [—É—Å—Ç–∞–ª], [—Ö–æ—Ä–æ—à–æ], [—á–µ–≥–æ].

        if emotions.get(face)!=None:
            emo = emotions.get(face)

        with open(f"characters\\{name}\\face\\{emo}.png", "rb") as f:
            face_base64 = base64.b64encode(f.read()).decode('ascii')
    print(emo)
    return jsonify({"content": answer_llm,"move": move,"tts": base64_tts, "face": face_base64,"emotion":emo})

@app.route('/chat/<name2>', methods=['POST'])
def chat(name2):
    data = request.get_json()
    #print(data)
    image=None
    move="stop"
    base64_tts=""
    answer_llm=""
    face=""
    face_base64=""
    answer_llm = request_llm_chat(data["messages"], name2)

    face = answer_llm[1]
    answer_llm=answer_llm[0]
    if bool(system_config.get("use_llm").lower()=="true"):
        with open("output_tts.wav", "rb") as f:
            base64_tts = base64.b64encode(f.read()).decode('ascii')
    if face!="":
        emotions={
            "—É–¥–∏–≤–ª—ë–Ω":"surprise",
            "–Ω–æ—Ä–º–∞–ª—å–Ω–æ": "normal",
            "–∑–ª–æ–π": "evil",
            "—Å–∫–µ–ø—Ç–∏—á–µ—Å–∫–∏–π": "sceptical",
            "—Ä–∞–¥–æ—Å—Ç—å": "happy",
            "—É—Å—Ç–∞–ª": "sleep",
            "—Ö–æ—Ä–æ—à–æ": "good",
            "—á–µ–≥–æ": "what",
        }
        if emotions.get(face)==None:
            file = "normal"
        else:
            file = emotions.get(face)

        with open(f"characters\\{name2}\\face\\{file}.png", "rb") as f:
            face_base64 = base64.b64encode(f.read()).decode('ascii')
    return jsonify({"content": answer_llm,"tts": base64_tts, "face": face_base64})


if __name__=="__main__":
    print("App running time:",time.time() - frist_time)
    app.run(service_config.get("host"),int(service_config.get("port")),threaded=True)

#
#
# if __name__=="__main__":
#         print("start")
#         char = Character("characters\\guy","test")
#         while True:
#             inp=input()
#             frist_time = time.time()
#             print(char.request(inp,"test2.jpg"))
#             print(time.time()-frist_time)